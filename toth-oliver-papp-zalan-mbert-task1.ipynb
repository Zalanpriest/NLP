{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10208961,"sourceType":"datasetVersion","datasetId":6309486},{"sourceId":10235189,"sourceType":"datasetVersion","datasetId":6328812}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport torch\nimport torch.nn as nn\nfrom transformers import get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom transformers import BertTokenizerFast, BertModel, AdamW\nfrom tqdm import tqdm\nimport os\n\n# ====================\n# Configuration\n# ====================\nDATA_PATH = \"/kaggle/input/nlp-task1-dataset/train.json\"  # Path to your data file\nVALIDATION_DATA_PATH = \"/kaggle/input/nlp-task1-dataset/validation.json\"\nMODEL_NAME = \"bert-base-multilingual-cased\"\nBATCH_SIZE = 8\nBERT_LEARNING_RATE = 1e-5\nCLASSIFIER_LEARNING_RATE = 5e-4\nEPOCHS_BINARY = 3\nEPOCHS_MULTI = 5\nMAX_LEN = 128\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\")\n\n# Suppose these are your 23 categories (you need to fill them according to your dataset)\n# We'll just define a small set from your snippet and dummy others.\nALL_PROP_CATS = [\n    'Logos',\n    'Repetition',\n    'Obfuscation, Intentional vagueness, Confusion',\n    'Reasoning',\n    'Justification',\n    'Slogans',\n    'Bandwagon',\n    'Appeal to authority',\n    'Flag-waving',\n    'Appeal to fear/prejudice',\n    'Simplification',\n    'Causal Oversimplification',\n    'Black-and-white Fallacy/Dictatorship',\n    'Thought-terminating clichÃ©',\n    'Distraction',\n    'Misrepresentation of Someone\\'s Position (Straw Man)',\n    'Presenting Irrelevant Data (Red Herring)',\n    'Whataboutism',\n    'Ethos',\n    'Glittering generalities (Virtue)',\n    'Ad Hominem',\n    'Doubt',\n    'Name calling/Labeling',\n    'Smears',\n    'Reductio ad hitlerum',\n    'Pathos',\n    'Exaggeration/Minimisation',\n    'Loaded Language'\n]\nassert len(ALL_PROP_CATS) == 28, \"You must have exactly 28 categories.\"\n\nCAT2IDX = {cat: i for i, cat in enumerate(ALL_PROP_CATS)}\n\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nfrom torch.nn.parallel import DistributedDataParallel as DDP\n\ndef setup_ddp():\n    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n        dist.init_process_group(backend='nccl', init_method='env://')\n        local_rank = int(os.environ['LOCAL_RANK'])  # Set by torchrun or your launcher\n        torch.cuda.set_device(local_rank)\n        return torch.device(f\"cuda:{local_rank}\")\n    else:\n        print(\"Distributed training not initialized. Using single GPU.\")\n        return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\n# ====================\n# Dataset\n# ====================\nclass PropagandaDataset(Dataset):\n    def __init__(self, data_path, tokenizer, max_len=128):\n        self.samples = []\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        \n        # Each item in data should have 'text' and 'labels' fields\n        for item in data:\n            text = item[\"text\"]\n            labels = item[\"labels\"]  # list of strings\n            \n            # Binary label: 1 if any label in labels is in ALL_PROP_CATS, else 0\n            is_propaganda = 1 if any(lbl in ALL_PROP_CATS for lbl in labels) else 0\n            \n            # Multi-class label: If propaganda, pick one label (or handle multi-label)\n            # If multiple categories exist, you might need a multi-hot vector or just pick the first.\n            # Here, we assume single-label classification; if multiple, you'd need another approach.\n            # We'll just pick the first if it exists.\n            if len(labels) > 0:\n                # Filter to known categories\n                known_cats = [lbl for lbl in labels if lbl in CAT2IDX]\n                if len(known_cats) > 0:\n                    # Multi-hot encoding for known categories\n                    multi_label = torch.zeros(len(CAT2IDX), dtype=torch.float32)\n                    for lbl in known_cats:\n                        multi_label[CAT2IDX[lbl]] = 1\n                else:\n                    # If none are recognized, assign an all-zero vector\n                    multi_label = torch.zeros(len(CAT2IDX), dtype=torch.float32)\n            else:\n                # Non-propaganda or no label assigned\n                multi_label = torch.zeros(len(CAT2IDX), dtype=torch.float32)\n            \n            enc = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n            input_ids = enc[\"input_ids\"].squeeze(0)\n            attention_mask = enc[\"attention_mask\"].squeeze(0)\n            \n            self.samples.append((input_ids, attention_mask, is_propaganda, multi_label))\n    \n    def __len__(self):\n        return len(self.samples)\n    \n    def __getitem__(self, idx):\n        return self.samples[idx]\n\n\n# ====================\n# Model\n# ====================\nclass HierarchicalPropClassifier(nn.Module):\n    def __init__(self, model_name=MODEL_NAME, num_prop_classes=28, dropout_prob=0.1):\n        super(HierarchicalPropClassifier, self).__init__()\n        self.bert = BertModel.from_pretrained(model_name)\n        \n        # Add a dropout layer\n        self.dropout = nn.Dropout(p=dropout_prob)\n        \n        self.binary_classifier = nn.Linear(self.bert.config.hidden_size, 2)\n        self.multi_classifier = nn.Linear(self.bert.config.hidden_size, num_prop_classes)\n        \n    def forward(self, input_ids, attention_mask, token_type_ids=None):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        cls_rep = outputs.last_hidden_state[:, 0, :]\n        \n        # Apply dropout to the CLS representation\n        cls_rep = self.dropout(cls_rep)\n        \n        binary_logits = self.binary_classifier(cls_rep)\n        multi_logits = self.multi_classifier(cls_rep)\n        \n        return binary_logits, multi_logits\n\n\n# ====================\n# Training Functions\n# ====================\ndef train_binary(model, dataloader, optimizer):\n    model.train()\n    criterion = nn.CrossEntropyLoss()\n    total_loss = 0\n    for batch in tqdm(dataloader, desc=\"Training Binary\"):\n        input_ids, attention_mask, binary_labels, multi_labels = [x.to(DEVICE) for x in batch]\n\n        optimizer.zero_grad()\n        binary_logits, multi_logits = model(input_ids, attention_mask=attention_mask)\n        \n        loss = criterion(binary_logits, binary_labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    return total_loss / len(dataloader)\n\ndef train_multi(model, dataloader, optimizer):\n    model.train()\n    criterion = nn.BCEWithLogitsLoss()  # For multi-hot encoding\n    total_loss = 0\n\n    for batch in tqdm(dataloader, desc=\"Training Multi\"):\n        input_ids, attention_mask, binary_labels, multi_labels = [x.to(DEVICE) for x in batch]\n\n        optimizer.zero_grad()\n\n        # Get logits from the model\n        _, multi_logits = model(input_ids, attention_mask=attention_mask)\n\n        # Compute loss directly using multi-hot labels\n        multi_loss = criterion(multi_logits, multi_labels)\n\n        # Backpropagation and optimization\n        multi_loss.backward()\n        optimizer.step()\n\n        total_loss += multi_loss.item()\n\n    return total_loss / len(dataloader)\n\ndef evaluate_binary(model, dataloader):\n    model.eval()\n    criterion = nn.CrossEntropyLoss()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, binary_labels, multi_labels = [x.to(DEVICE) for x in batch]\n            binary_logits, _ = model(input_ids, attention_mask=attention_mask)\n            loss = criterion(binary_logits, binary_labels)\n            total_loss += loss.item()\n            preds = binary_logits.argmax(dim=1)\n            correct += (preds == binary_labels).sum().item()\n            total += binary_labels.size(0)\n    return total_loss / len(dataloader), correct / total\n\ndef evaluate_multi(model, dataloader):\n    model.eval()\n    criterion = nn.CrossEntropyLoss()\n    total_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids, attention_mask, binary_labels, multi_labels = [x.to(DEVICE) for x in batch]\n            binary_logits, multi_logits = model(input_ids, attention_mask=attention_mask)\n            \n            # Apply sigmoid activation to logits\n            multi_probs = torch.sigmoid(multi_logits)\n            \n            # Use threshold to decide active categories\n            preds = (multi_probs > 0.5).long()\n            \n            # Calculate multi-label loss\n            loss = criterion(multi_logits, multi_labels)\n            total_loss += loss.item()\n            \n            # For evaluation: compare predictions and true labels\n            correct += (preds == multi_labels.long()).sum().item()\n            total += multi_labels.numel()\n    if total == 0:\n        return 0, 0\n    return total_loss / len(dataloader), correct / total\n\n\n# ====================\n# Main\n# ====================\n\nDEVICE = setup_ddp()\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\ntrain_dataset = PropagandaDataset(DATA_PATH, tokenizer, max_len=MAX_LEN)\nval_dataset = PropagandaDataset(VALIDATION_DATA_PATH, tokenizer, max_len=MAX_LEN)\n\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\nmodel = HierarchicalPropClassifier().to(DEVICE)\nif \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n    # Distributed mode\n    model = DDP(model, device_ids=[DEVICE.index])\nelif torch.cuda.device_count() > 1:\n    # DataParallel mode\n    model = nn.DataParallel(model)\nelse:\n    # Single GPU\n    pass\noptimizer = AdamW([\n    {\"params\": model.bert.parameters(), \"lr\": BERT_LEARNING_RATE},  # BERT parameters\n    {\"params\": model.binary_classifier.parameters(), \"lr\": CLASSIFIER_LEARNING_RATE},  # Binary classification head\n    {\"params\": model.multi_classifier.parameters(), \"lr\": CLASSIFIER_LEARNING_RATE},   # Multi-class classification head\n], weight_decay=1e-2)\n\n# Total number of training steps\ntotal_steps = len(train_loader) * EPOCHS_BINARY  # Adjust based on training phase\n\n# Scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=0,  # Number of steps to warm up the learning rate\n    num_training_steps=total_steps\n)\n\n# Phase 1: Train binary classifier\nprint(\"=== Phase 1: Binary Classification Training ===\")\nfor epoch in range(EPOCHS_BINARY):\n    train_loss = train_binary(model, train_loader, optimizer)\n    val_loss, val_acc = evaluate_binary(model, val_loader)\n    print(f\"Epoch {epoch+1}/{EPOCHS_BINARY} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n# Phase 2: Train multi-class classifier (for propagandistic samples)\nprint(\"=== Phase 2: Multi-Class Classification Training ===\")\nfor epoch in range(EPOCHS_MULTI):\n    train_loss = train_multi(model, train_loader, optimizer)\n    val_loss, val_acc = evaluate_multi(model, val_loader)\n    print(f\"Epoch {epoch+1}/{EPOCHS_MULTI} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n\n# You can now use model for inference.\n# Example inference:\nmodel.eval()\ntext = \"THIS IS WHY YOU NEED A SHARPIE WITH YOU AT ALL TIMES\"\ninputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\nwith torch.no_grad():\n    inputs.pop(\"token_type_ids\")\n    binary_logits, multi_logits = model(**inputs)\nbinary_pred = binary_logits.argmax(dim=1).item()\nif binary_pred == 1:\n    # It's propaganda, check which category\n    # Apply sigmoid activation\n    multi_probs = torch.sigmoid(multi_logits)\n    \n    # Apply threshold (e.g., 0.5)\n    multi_pred = (multi_probs > 0.5).nonzero(as_tuple=True)[1]\n    \n    # Get predicted categories\n    predicted_categories = [ALL_PROP_CATS[idx] for idx in multi_pred.tolist()]\n    print(f\"Prediction: Propaganda ({', '.join(predicted_categories)})\")\nelse:\n    print(\"Prediction: Non-Propaganda\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def test_model(model, dataloader, task_type=\"binary\"):\n    \"\"\"\n    Function to evaluate the model for binary or multi-label classification.\n\n    Parameters:\n    - model: The trained model.\n    - dataloader: DataLoader containing the test dataset.\n    - task_type: The task type ('binary' for binary classification, 'multi' for multi-label classification).\n\n    Returns:\n    - avg_loss: Average loss over all batches.\n    - accuracy: Accuracy for binary classification, or multi-label accuracy for multi-label.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    # Define the appropriate loss function based on the task type\n    if task_type == \"binary\":\n        criterion = nn.CrossEntropyLoss()  # For binary classification\n    elif task_type == \"multi\":\n        criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n    else:\n        raise ValueError(\"Invalid task type. Use 'binary' or 'multi'.\")\n\n    with torch.no_grad():  # No need to calculate gradients during testing\n        for batch in tqdm(dataloader, desc=f\"Evaluating {task_type.capitalize()}\"):\n            input_ids, attention_mask, binary_labels, multi_labels = [x.to(DEVICE) for x in batch]\n            \n            # Get model predictions\n            binary_logits, multi_logits = model(input_ids, attention_mask=attention_mask)\n\n            # Calculate loss and accuracy\n            if task_type == \"binary\":\n                loss = criterion(binary_logits, binary_labels)\n                total_loss += loss.item()\n                preds = binary_logits.argmax(dim=1)  # Get the predicted class for binary\n                correct += (preds == binary_labels).sum().item()\n                total += binary_labels.size(0)\n\n            elif task_type == \"multi\":\n                loss = criterion(multi_logits, multi_labels)  # Multi-label loss\n                total_loss += loss.item()\n                \n                # Apply sigmoid to multi-label logits and get predicted classes\n                multi_probs = torch.sigmoid(multi_logits)\n                preds = (multi_probs > 0.5).long()  # Threshold at 0.5 for multi-label\n                \n                # Count correct multi-label predictions\n                correct += (preds == multi_labels.long()).sum().item()\n                total += multi_labels.numel()  # Count total labels\n\n    # Calculate accuracy or average accuracy for multi-label task\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct / total\n\n    return avg_loss, accuracy\n\nbinary_test_loss, binary_test_acc = test_model(model, val_loader, task_type=\"binary\")\nprint(f\"Binary Test Loss: {binary_test_loss:.4f}, Binary Test Accuracy: {binary_test_acc:.4f}\")\n\nmulti_test_loss, multi_test_acc = test_model(model, val_loader, task_type=\"multi\")\nprint(f\"Multi-label Test Loss: {multi_test_loss:.4f}, Multi-label Test Accuracy: {multi_test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def predict_text(model, tokenizer, text, device=DEVICE, threshold=0.5):\n    \"\"\"\n    Function to predict whether a text is propaganda and the categories it belongs to.\n\n    Parameters:\n    - model: The trained model.\n    - tokenizer: The tokenizer for encoding the input text.\n    - text: The input text to predict.\n    - device: The device to run the model on (default is the device defined in the environment).\n    - threshold: The threshold for multi-label classification to decide if a category is present.\n\n    Returns:\n    - None: Prints the results to the console.\n    \"\"\"\n    model.eval()  # Set the model to evaluation mode\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=MAX_LEN).to(device)\n    \n    # Forward pass: Get model outputs\n    with torch.no_grad():\n        binary_logits, multi_logits = model(**inputs)\n    \n    # Get binary prediction\n    binary_pred = binary_logits.argmax(dim=1).item()\n    \n    # Print binary classification result\n    if binary_pred == 1:\n        print(\"Prediction: Propaganda\")\n    else:\n        print(\"Prediction: Non-Propaganda\")\n    \n    # Get multi-label predictions (apply sigmoid and threshold)\n    multi_probs = torch.sigmoid(multi_logits)\n    multi_pred = (multi_probs > threshold).nonzero(as_tuple=True)[1].tolist()\n    \n    if multi_pred:\n        predicted_categories = [ALL_PROP_CATS[idx] for idx in multi_pred]\n        print(f\"Predicted Categories: {', '.join(predicted_categories)}\")\n    else:\n        print(\"No categories predicted for this text.\")\n\n\ntext = \"THE GREAT (FAKE) CHILD- SEX-TRAFFICKING EPIDEMIC\\\\nLotrene Powel obs Owner of the Atlantic\\\\nGhislaine Maxwell\\\\nEpstein's partner in crime\"\npredict_text(model, tokenizer, text)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nclass PropagandaDataset(Dataset):\n    def __init__(self, data_path, tokenizer, max_len=128):\n        self.samples = []\n        with open(data_path, \"r\", encoding=\"utf-8\") as f:\n            data = json.load(f)\n        \n        # Each item in data should have 'text' and 'labels' fields\n        for item in data:\n            text = item[\"text\"]\n            labels = item[\"labels\"]  # list of strings\n            \n            # Binary label: 1 if any label in labels is in ALL_PROP_CATS, else 0\n            is_propaganda = 1 if any(lbl in ALL_PROP_CATS for lbl in labels) else 0\n            \n            # Multi-class label: If propaganda, pick one label (or handle multi-label)\n            # If multiple categories exist, you might need a multi-hot vector or just pick the first.\n            # Here, we assume single-label classification; if multiple, you'd need another approach.\n            # We'll just pick the first if it exists.\n            if len(labels) > 0:\n                # Filter to known categories\n                known_cats = [lbl for lbl in labels if lbl in CAT2IDX]\n                if len(known_cats) > 0:\n                    # Multi-hot encoding for known categories\n                    multi_label = torch.zeros(len(CAT2IDX), dtype=torch.float32)\n                    for lbl in known_cats:\n                        multi_label[CAT2IDX[lbl]] = 1\n                else:\n                    # If none are recognized, assign an all-zero vector\n                    multi_label = torch.zeros(len(CAT2IDX), dtype=torch.float32)\n            else:\n                # Non-propaganda or no label assigned\n                multi_label = torch.zeros(len(CAT2IDX), dtype=torch.float32)\n            \n            enc = tokenizer(text, truncation=True, padding=\"max_length\", max_length=max_len, return_tensors=\"pt\")\n            input_ids = enc[\"input_ids\"].squeeze(0)\n            attention_mask = enc[\"attention_mask\"].squeeze(0)\n            \n            self.samples.append((input_ids, attention_mask, is_propaganda, multi_label))\n    \ndef test_model(model, dataloader, task_type=\"binary\"):\n    \"\"\"\n    Function to evaluate the model for binary or multi-label classification.\n\n    Parameters:\n    - model: The trained model.\n    - dataloader: DataLoader containing the test dataset.\n    - task_type: The task type ('binary' for binary classification, 'multi' for multi-label classification).\n\n    Returns:\n    - avg_loss: Average loss over all batches.\n    - accuracy: Accuracy for binary classification, or multi-label accuracy for multi-label.\n    \"\"\"\n    model.eval()  # Set model to evaluation mode\n    total_loss = 0\n    correct = 0\n    total = 0\n\n    # Define the appropriate loss function based on the task type\n    if task_type == \"binary\":\n        criterion = nn.CrossEntropyLoss()  # For binary classification\n    elif task_type == \"multi\":\n        criterion = nn.BCEWithLogitsLoss()  # For multi-label classification\n    else:\n        raise ValueError(\"Invalid task type. Use 'binary' or 'multi'.\")\n\n    with torch.no_grad():  # No need to calculate gradients during testing\n        for batch in tqdm(dataloader, desc=f\"Evaluating {task_type.capitalize()}\"):\n            input_ids, attention_mask, binary_labels, multi_labels = [x.to(DEVICE) for x in batch]\n            \n            # Get model predictions\n            binary_logits, multi_logits = model(input_ids, attention_mask=attention_mask)\n\n            # Calculate loss and accuracy\n            if task_type == \"binary\":\n                loss = criterion(binary_logits, binary_labels)\n                total_loss += loss.item()\n                preds = binary_logits.argmax(dim=1)  # Get the predicted class for binary\n                correct += (preds == binary_labels).sum().item()\n                total += binary_labels.size(0)\n\n            elif task_type == \"multi\":\n                loss = criterion(multi_logits, multi_labels)  # Multi-label loss\n                total_loss += loss.item()\n                \n                # Apply sigmoid to multi-label logits and get predicted classes\n                multi_probs = torch.sigmoid(multi_logits)\n                preds = (multi_probs > 0.5).long()  # Threshold at 0.5 for multi-label\n                \n                # Count correct multi-label predictions\n                correct += (preds == multi_labels.long()).sum().item()\n                total += multi_labels.numel()  # Count total labels\n\n    # Calculate accuracy or average accuracy for multi-label task\n    avg_loss = total_loss / len(dataloader)\n    accuracy = correct / total\n\n    return avg_loss, accuracy\n\nVALIDATION2_DATA_PATH = \"/kaggle/input/lang-test/test_subtask1_ar.json\"\n\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n\nval2_dataset = PropagandaDataset(VALIDATION2_DATA_PATH, tokenizer, max_len=MAX_LEN)\n\nval2_loader = DataLoader(val2_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\nbinary_test_loss, binary_test_acc = test_model(model, val_loader, task_type=\"binary\")\nprint(f\"Binary Test Loss: {binary_test_loss:.4f}, Binary Test Accuracy: {binary_test_acc:.4f}\")\n\nmulti_test_loss, multi_test_acc = test_model(model, val_loader, task_type=\"multi\")\nprint(f\"Multi-label Test Loss: {multi_test_loss:.4f}, Multi-label Test Accuracy: {multi_test_acc:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}